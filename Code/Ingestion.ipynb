{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5da8a28b-18d5-4944-916e-4ce419ba6d90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Install Kaggle dependecies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76820508-c010-4444-ae09-7de8916f89a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kaggle in /local_disk0/.ephemeral_nfs/envs/pythonEnv-fb8cdccb-6bbd-4361-a827-d2cd5db36523/lib/python3.11/site-packages (1.7.4.2)\nRequirement already satisfied: azure-storage-blob in /databricks/python3/lib/python3.11/site-packages (12.19.1)\nRequirement already satisfied: bleach in /local_disk0/.ephemeral_nfs/envs/pythonEnv-fb8cdccb-6bbd-4361-a827-d2cd5db36523/lib/python3.11/site-packages (from kaggle) (6.2.0)\nRequirement already satisfied: certifi>=14.05.14 in /databricks/python3/lib/python3.11/site-packages (from kaggle) (2023.7.22)\nRequirement already satisfied: charset-normalizer in /databricks/python3/lib/python3.11/site-packages (from kaggle) (2.0.4)\nRequirement already satisfied: idna in /databricks/python3/lib/python3.11/site-packages (from kaggle) (3.4)\nRequirement already satisfied: protobuf in /databricks/python3/lib/python3.11/site-packages (from kaggle) (5.29.3)\nRequirement already satisfied: python-dateutil>=2.5.3 in /databricks/python3/lib/python3.11/site-packages (from kaggle) (2.8.2)\nRequirement already satisfied: python-slugify in /local_disk0/.ephemeral_nfs/envs/pythonEnv-fb8cdccb-6bbd-4361-a827-d2cd5db36523/lib/python3.11/site-packages (from kaggle) (8.0.4)\nRequirement already satisfied: requests in /databricks/python3/lib/python3.11/site-packages (from kaggle) (2.31.0)\nRequirement already satisfied: setuptools>=21.0.0 in /databricks/python3/lib/python3.11/site-packages (from kaggle) (75.1.0)\nRequirement already satisfied: six>=1.10 in /usr/lib/python3/dist-packages (from kaggle) (1.16.0)\nRequirement already satisfied: text-unidecode in /local_disk0/.ephemeral_nfs/envs/pythonEnv-fb8cdccb-6bbd-4361-a827-d2cd5db36523/lib/python3.11/site-packages (from kaggle) (1.3)\nRequirement already satisfied: tqdm in /local_disk0/.ephemeral_nfs/envs/pythonEnv-fb8cdccb-6bbd-4361-a827-d2cd5db36523/lib/python3.11/site-packages (from kaggle) (4.67.1)\nRequirement already satisfied: urllib3>=1.15.1 in /databricks/python3/lib/python3.11/site-packages (from kaggle) (1.26.16)\nRequirement already satisfied: webencodings in /local_disk0/.ephemeral_nfs/envs/pythonEnv-fb8cdccb-6bbd-4361-a827-d2cd5db36523/lib/python3.11/site-packages (from kaggle) (0.5.1)\nRequirement already satisfied: azure-core<2.0.0,>=1.28.0 in /databricks/python3/lib/python3.11/site-packages (from azure-storage-blob) (1.32.0)\nRequirement already satisfied: cryptography>=2.1.4 in /databricks/python3/lib/python3.11/site-packages (from azure-storage-blob) (41.0.3)\nRequirement already satisfied: typing-extensions>=4.3.0 in /databricks/python3/lib/python3.11/site-packages (from azure-storage-blob) (4.10.0)\nRequirement already satisfied: isodate>=0.6.1 in /databricks/python3/lib/python3.11/site-packages (from azure-storage-blob) (0.7.2)\nRequirement already satisfied: cffi>=1.12 in /databricks/python3/lib/python3.11/site-packages (from cryptography>=2.1.4->azure-storage-blob) (1.15.1)\nRequirement already satisfied: pycparser in /databricks/python3/lib/python3.11/site-packages (from cffi>=1.12->cryptography>=2.1.4->azure-storage-blob) (2.21)\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install kaggle azure-storage-blob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "28e3a37f-2660-4958-859e-1a863850bbdb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01738fc0-7f95-4bc8-9b1c-a22d61799d8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "from pyspark.dbutils import DBUtils\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum\n",
    "from pyspark.sql.functions import col, expr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a9cc936-f607-4dec-b337-c9b0d035b5ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Downloading raw file from Kaggle and puttting to Azure datalake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d173c67-34be-4a74-ad53-c49b9a1b818a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset written as Parquet file with partitions successfully!\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Import necessary libraries\n",
    "import os\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "from pyspark.sql.functions import col, year, month, dayofmonth, current_date\n",
    "\n",
    "# Cell 2: Configurations\n",
    "KAGGLE_USERNAME = KAGGLE_USERNAME  \n", 
    "KAGGLE_KEY = KAGGLE_KEY\n, #This located under Github Repository secrets",
    "AZURE_STORAGE_ACCOUNT_NAME = noviprojectst'\n",
    "AZURE_STORAGE_ACCOUNT_KEY = AZURE_STORAGE_ACCOUNT_KEY,  #This located under Github Repository secrets",
    "CONTAINER_NAME = 'incoming'\n",
    "DATASET_PATH = 'jayaantanaath/student-habits-vs-academic-performance'\n",
    "\n",
    "# Set Kaggle credentials as environment variables\n",
    "os.environ['KAGGLE_USERNAME'] = KAGGLE_USERNAME\n",
    "os.environ['KAGGLE_KEY'] = KAGGLE_KEY\n",
    "\n",
    "# Authenticate with Kaggle\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "\n",
    "# Download dataset\n",
    "api.dataset_download_files(DATASET_PATH, path='/dbfs/tmp/data', unzip=True)\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"KaggleDataToParquet\").getOrCreate()\n",
    "\n",
    "# Set Spark properties to configure Azure credentials\n",
    "spark.conf.set(f\"fs.azure.account.key.{AZURE_STORAGE_ACCOUNT_NAME}.blob.core.windows.net\", AZURE_STORAGE_ACCOUNT_KEY)\n",
    "\n",
    "# Read the dataset into a Spark DataFrame\n",
    "csv_file_path = '/dbfs/tmp/data/Student_Habits_and_Academic_Performance.csv'\n",
    "df = spark.read.csv(csv_file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Add current year, month, and day columns\n",
    "df = df.withColumn(\"year\", year(current_date())) \\\n",
    "       .withColumn(\"month\", month(current_date())) \\\n",
    "       .withColumn(\"day\", dayofmonth(current_date()))\n",
    "\n",
    "# Write the DataFrame as a Parquet file\n",
    "output_path = f\"wasbs://{CONTAINER_NAME}@{AZURE_STORAGE_ACCOUNT_NAME}.blob.core.windows.net/parquet_output\"\n",
    "df.write.partitionBy(\"year\", \"month\", \"day\").parquet(output_path)\n",
    "\n",
    "print(\"Dataset written as Parquet file with partitions successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b3eedba1-ac32-4230-98a6-a3df5ca015c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
